{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05ced366",
   "metadata": {},
   "source": [
    "# Fourier Phase Retrieval - Kaggle Master Notebook\n",
    "\n",
    "This notebook consolidates the full pipeline:\n",
    "- Dataset construction (MNIST -> diffraction intensity)\n",
    "- U-Net model (DoubleConv blocks)\n",
    "- Physics forward model (Bartlett window + FFT pipeline)\n",
    "- Pre-training\n",
    "- Test-time fine-tuning\n",
    "- Evaluation and metrics\n",
    "- Export results/models as a zip file\n",
    "\n",
    "Use the `MODE` config cell to switch between:\n",
    "- `TEST`   (500 train / 50 val, 1 epoch)\n",
    "- `MEDIUM` (25% train / 25% val, 10 epochs)\n",
    "- `FULL`   (100% train / 100% val, 30 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab09929a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.12.10)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/cohen/Desktop/Fourier-Phase-Retrieval-Physics-DL/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from IPython.display import FileLink, display\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "BASE_DIR = Path('.')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "RESULTS_DIR = BASE_DIR / 'results'\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ff33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Configuration: TEST / MEDIUM / FULL\n",
    "# =============================\n",
    "MODE = 'TEST'  # Change to 'MEDIUM' or 'FULL'\n",
    "\n",
    "CONFIGS = {\n",
    "    'TEST': {\n",
    "        'train_size': 500,\n",
    "        'val_size': 50,\n",
    "        'epochs': 1,\n",
    "        'batch_size': 16,\n",
    "        'finetune_iterations': 20,\n",
    "        'eval_samples': 2,\n",
    "        'lr_pretrain': 1e-3,\n",
    "        'lr_finetune': 1e-4,\n",
    "        'early_stopping_patience': 3,\n",
    "        'scheduler_patience': 1,\n",
    "    },\n",
    "    'MEDIUM': {\n",
    "        'train_fraction': 0.25,\n",
    "        'val_fraction': 0.25,\n",
    "        'epochs': 10,\n",
    "        'batch_size': 16,\n",
    "        'finetune_iterations': 50,\n",
    "        'eval_fraction': 0.25,\n",
    "        'max_eval_samples': 250,\n",
    "        'lr_pretrain': 1e-3,\n",
    "        'lr_finetune': 1e-4,\n",
    "        'early_stopping_patience': 4,\n",
    "        'scheduler_patience': 2,\n",
    "    },\n",
    "    'FULL': {\n",
    "        'train_fraction': 1.0,\n",
    "        'val_fraction': 1.0,\n",
    "        'epochs': 30,\n",
    "        'batch_size': 16,\n",
    "        'finetune_iterations': 100,\n",
    "        'eval_fraction': 1.0,\n",
    "        'max_eval_samples': 500,\n",
    "        'lr_pretrain': 1e-3,\n",
    "        'lr_finetune': 1e-4,\n",
    "        'early_stopping_patience': 6,\n",
    "        'scheduler_patience': 3,\n",
    "    },\n",
    "}\n",
    "\n",
    "cfg = CONFIGS[MODE]\n",
    "print('MODE:', MODE)\n",
    "print('Config:', cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca11d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Dataset: MNIST -> Diffraction Intensity (Windowed + Log-scaled)\n",
    "# =============================\n",
    "class FPRDataset(Dataset):\n",
    "    def __init__(self, mnist_root='./data', train=True, size=128):\n",
    "        self.mnist = datasets.MNIST(\n",
    "            root=mnist_root,\n",
    "            train=train,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.Resize(size),\n",
    "                transforms.ToTensor(),\n",
    "            ]),\n",
    "        )\n",
    "        w1d = torch.bartlett_window(size)\n",
    "        self.window = w1d.unsqueeze(1) * w1d.unsqueeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist)\n",
    "\n",
    "    def compute_diffraction(self, image):\n",
    "        windowed_image = image[0] * self.window\n",
    "        fourier = torch.fft.fft2(windowed_image)\n",
    "        fourier_shifted = torch.fft.fftshift(fourier, dim=(-2, -1))\n",
    "        intensity = torch.abs(fourier_shifted) ** 2\n",
    "        log_intensity = torch.log1p(intensity)\n",
    "        normalized = (log_intensity - log_intensity.min()) / (log_intensity.max() - log_intensity.min() + 1e-12)\n",
    "        return normalized.unsqueeze(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, _ = self.mnist[idx]\n",
    "        intensity = self.compute_diffraction(image)\n",
    "        return intensity, image\n",
    "\n",
    "\n",
    "def build_subsets(dataset_train, dataset_val, config):\n",
    "    n_train = len(dataset_train)\n",
    "    n_val = len(dataset_val)\n",
    "\n",
    "    if 'train_size' in config:\n",
    "        train_size = min(config['train_size'], n_train)\n",
    "    else:\n",
    "        train_size = int(n_train * config['train_fraction'])\n",
    "\n",
    "    if 'val_size' in config:\n",
    "        val_size = min(config['val_size'], n_val)\n",
    "    else:\n",
    "        val_size = int(n_val * config['val_fraction'])\n",
    "\n",
    "    train_subset = Subset(dataset_train, list(range(train_size)))\n",
    "    val_subset = Subset(dataset_val, list(range(val_size)))\n",
    "    return train_subset, val_subset\n",
    "\n",
    "\n",
    "full_train = FPRDataset(mnist_root=str(DATA_DIR), train=True, size=128)\n",
    "full_val = FPRDataset(mnist_root=str(DATA_DIR), train=False, size=128)\n",
    "train_subset, val_subset = build_subsets(full_train, full_val, cfg)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=cfg['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_subset,\n",
    "    batch_size=cfg['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f'Train samples: {len(train_subset):,}')\n",
    "print(f'Val samples: {len(val_subset):,}')\n",
    "\n",
    "sample_inp, sample_tgt = train_subset[0]\n",
    "print('Input shape:', sample_inp.shape, '| Target shape:', sample_tgt.shape)\n",
    "print(f'Input abs min/max: {sample_inp.abs().min().item():.6f} / {sample_inp.abs().max().item():.6f}')\n",
    "print(f'Target abs min/max: {sample_tgt.abs().min().item():.6f} / {sample_tgt.abs().max().item():.6f}')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax[0].imshow(sample_inp[0].cpu().numpy(), cmap='viridis')\n",
    "ax[0].set_title('Sample Input (Diffraction)')\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(sample_tgt[0].cpu().numpy(), cmap='gray')\n",
    "ax[1].set_title('Sample Target (Image)')\n",
    "ax[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec3b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# U-Net Architecture\n",
    "# =============================\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc1 = DoubleConv(1, 32)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = DoubleConv(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = DoubleConv(64, 128)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.enc4 = DoubleConv(128, 256)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = DoubleConv(256, 512)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec4 = DoubleConv(512, 256)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec3 = DoubleConv(256, 128)\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec2 = DoubleConv(128, 64)\n",
    "        self.up1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.dec1 = DoubleConv(64, 32)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(32, 1, kernel_size=3, padding=1)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "        e4 = self.enc4(self.pool3(e3))\n",
    "\n",
    "        b = self.bottleneck(self.pool4(e4))\n",
    "\n",
    "        d4 = self.up4(b)\n",
    "        d4 = self.dec4(torch.cat([d4, e4], dim=1))\n",
    "        d3 = self.up3(d4)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = self.dec1(torch.cat([d1, e1], dim=1))\n",
    "\n",
    "        out = self.out_act(self.out_conv(d1))\n",
    "        return out\n",
    "\n",
    "\n",
    "model = UNet().to(device)\n",
    "x = torch.randn(2, 1, 128, 128, device=device)\n",
    "y = model(x)\n",
    "print('Model OK. Output shape:', y.shape)\n",
    "print('Parameters:', f\"{sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9083ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Physics Forward Model + Fine-tuning\n",
    "# =============================\n",
    "def bartlett_window(size, device=device):\n",
    "    w = torch.bartlett_window(size, device=device)\n",
    "    return w.unsqueeze(1) * w.unsqueeze(0)\n",
    "\n",
    "\n",
    "def physics_forward(image, window):\n",
    "    # Supports [B,1,H,W] and [1,H,W]\n",
    "    if image.dim() == 4:\n",
    "        img = image[:, 0]\n",
    "        windowed = img * window.unsqueeze(0)\n",
    "        fourier = torch.fft.fft2(windowed)\n",
    "        shifted = torch.fft.fftshift(fourier)\n",
    "        intensity = torch.abs(shifted) ** 2\n",
    "        log_intensity = torch.log1p(intensity)\n",
    "        min_v = log_intensity.amin(dim=(-2, -1), keepdim=True)\n",
    "        max_v = log_intensity.amax(dim=(-2, -1), keepdim=True)\n",
    "        norm = (log_intensity - min_v) / (max_v - min_v + 1e-12)\n",
    "        return norm.unsqueeze(1)\n",
    "    if image.dim() == 3:\n",
    "        img = image[0]\n",
    "        windowed = img * window\n",
    "        fourier = torch.fft.fft2(windowed)\n",
    "        shifted = torch.fft.fftshift(fourier)\n",
    "        intensity = torch.abs(shifted) ** 2\n",
    "        log_intensity = torch.log1p(intensity)\n",
    "        norm = (log_intensity - log_intensity.min()) / (log_intensity.max() - log_intensity.min() + 1e-12)\n",
    "        return norm.unsqueeze(0)\n",
    "    raise ValueError('Unsupported image shape for physics_forward')\n",
    "\n",
    "\n",
    "def finetune_test_sample(model, measured_intensity, window, iterations=100, lr=1e-4, lambda_tv=1e-5):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        pred_image = model(measured_intensity)\n",
    "        pred_intensity = physics_forward(pred_image, window)\n",
    "\n",
    "        tv_h = torch.abs(pred_image[:, :, 1:, :] - pred_image[:, :, :-1, :]).sum()\n",
    "        tv_w = torch.abs(pred_image[:, :, :, 1:] - pred_image[:, :, :, :-1]).sum()\n",
    "        tv_loss = tv_h + tv_w\n",
    "\n",
    "        data_loss = criterion(pred_intensity, measured_intensity)\n",
    "        total_loss = data_loss + lambda_tv * tv_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        losses.append(total_loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        refined = model(measured_intensity).detach()\n",
    "    return refined, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b9be42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Pre-training (Train + Validation)\n",
    "# =============================\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg['lr_pretrain'], weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=cfg['scheduler_patience'],\n",
    ")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Save policy:\n",
    "# Every 5 epochs -> keep best model so far + 3 latest periodic checkpoints\n",
    "checkpoint_every = 5\n",
    "recent_checkpoints = []\n",
    "best_val = float('inf')\n",
    "best_path = MODELS_DIR / 'best_model_so_far.pth'\n",
    "\n",
    "patience = cfg['early_stopping_patience']\n",
    "early_stop_counter = 0\n",
    "best_for_patience = float('inf')\n",
    "\n",
    "for epoch in range(1, cfg['epochs'] + 1):\n",
    "    model.train()\n",
    "    train_sum = 0.0\n",
    "\n",
    "    train_bar = tqdm(train_loader, desc=f'Epoch {epoch:02d} Train', leave=False)\n",
    "    for inp, tgt in train_bar:\n",
    "        inp = inp.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        pred = model(inp)\n",
    "        loss = criterion(pred, tgt)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_sum += loss.item()\n",
    "        train_bar.set_postfix({'loss': f'{loss.item():.5f}'})\n",
    "\n",
    "    avg_train = train_sum / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_sum = 0.0\n",
    "    val_bar = tqdm(val_loader, desc=f'Epoch {epoch:02d} Val', leave=False)\n",
    "    with torch.no_grad():\n",
    "        for inp, tgt in val_bar:\n",
    "            inp = inp.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            pred = model(inp)\n",
    "            batch_val = criterion(pred, tgt).item()\n",
    "            val_sum += batch_val\n",
    "            val_bar.set_postfix({'val_loss': f'{batch_val:.5f}'})\n",
    "\n",
    "    avg_val = val_sum / len(val_loader)\n",
    "\n",
    "    train_losses.append(avg_train)\n",
    "    val_losses.append(avg_val)\n",
    "\n",
    "    scheduler.step(avg_val)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # Track and save best model so far (always up-to-date)\n",
    "    if avg_val < best_val:\n",
    "        best_val = avg_val\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'val_loss': avg_val,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'mode': MODE,\n",
    "            },\n",
    "            best_path,\n",
    "        )\n",
    "\n",
    "    # Every 5 epochs: save periodic checkpoint and keep only 3 latest\n",
    "    if epoch % checkpoint_every == 0:\n",
    "        ckpt_path = MODELS_DIR / f'last_run_epoch_{epoch:02d}.pth'\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'val_loss': avg_val,\n",
    "                'best_val_so_far': best_val,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'mode': MODE,\n",
    "            },\n",
    "            ckpt_path,\n",
    "        )\n",
    "\n",
    "        recent_checkpoints.append(ckpt_path)\n",
    "        if len(recent_checkpoints) > 3:\n",
    "            oldest = recent_checkpoints.pop(0)\n",
    "            if oldest.exists():\n",
    "                oldest.unlink()\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val < best_for_patience:\n",
    "        best_for_patience = avg_val\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch:02d}/{cfg[\"epochs\"]} | '\n",
    "        f'Train: {avg_train:.6f} | Val: {avg_val:.6f} | '\n",
    "        f'Best Val: {best_val:.6f} | LR: {current_lr:.2e} | '\n",
    "        f'Patience: {early_stop_counter}/{patience}'\n",
    "    )\n",
    "\n",
    "    if early_stop_counter >= patience:\n",
    "        print(f'Early stopping triggered at epoch {epoch}.')\n",
    "        break\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_losses, marker='o', label='Train Loss')\n",
    "plt.plot(val_losses, marker='s', label='Val Loss')\n",
    "plt.title(f'Pre-training Curves ({MODE})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / f'pretrain_curves_{MODE.lower()}.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print('Best model so far:', best_path)\n",
    "print('Kept latest periodic checkpoints (max 3):')\n",
    "for p in recent_checkpoints:\n",
    "    print(' -', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Evaluation: Pre-trained vs Fine-tuned\n",
    "# =============================\n",
    "model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "window = bartlett_window(128, device=device)\n",
    "\n",
    "if MODE == 'TEST':\n",
    "    eval_indices = list(range(min(cfg['eval_samples'], len(val_subset))))\n",
    "else:\n",
    "    fraction = cfg.get('eval_fraction', 0.25)\n",
    "    max_eval = cfg.get('max_eval_samples', len(val_subset))\n",
    "    n_eval = min(int(len(val_subset) * fraction), max_eval)\n",
    "    n_eval = max(n_eval, 1)\n",
    "    eval_indices = list(range(n_eval))\n",
    "\n",
    "rows = []\n",
    "viz_cap = min(8, len(eval_indices))\n",
    "fig, axes = plt.subplots(viz_cap, 4, figsize=(14, 3 * viz_cap))\n",
    "if viz_cap == 1:\n",
    "    axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "ft_loss_traces = []\n",
    "\n",
    "for r, idx in enumerate(tqdm(eval_indices, desc='Fine-tune eval')):\n",
    "    measured_intensity, target = val_subset[idx]\n",
    "    measured_intensity = measured_intensity.unsqueeze(0).to(device)\n",
    "    target = target.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pretrained_pred = model(measured_intensity)\n",
    "\n",
    "    mse_pre = nn.functional.mse_loss(pretrained_pred, target).item()\n",
    "\n",
    "    finetuned_pred, ft_losses = finetune_test_sample(\n",
    "        model=model,\n",
    "        measured_intensity=measured_intensity,\n",
    "        window=window,\n",
    "        iterations=cfg['finetune_iterations'],\n",
    "        lr=cfg['lr_finetune'],\n",
    "    )\n",
    "\n",
    "    mse_ft = nn.functional.mse_loss(finetuned_pred, target).item()\n",
    "    improvement = (mse_pre - mse_ft) / (mse_pre + 1e-12) * 100.0\n",
    "\n",
    "    ft_loss_traces.append({'sample_id': idx, 'losses': ft_losses})\n",
    "\n",
    "    rows.append({\n",
    "        'sample_id': idx,\n",
    "        'mse_pretrained': mse_pre,\n",
    "        'mse_finetuned': mse_ft,\n",
    "        'improvement_percent': improvement,\n",
    "    })\n",
    "\n",
    "    if r < viz_cap:\n",
    "        axes[r, 0].imshow(measured_intensity[0, 0].detach().cpu().numpy(), cmap='viridis')\n",
    "        axes[r, 0].set_title(f'Sample {idx} Input')\n",
    "        axes[r, 0].axis('off')\n",
    "\n",
    "        axes[r, 1].imshow(target[0, 0].detach().cpu().numpy(), cmap='gray')\n",
    "        axes[r, 1].set_title('Ground Truth')\n",
    "        axes[r, 1].axis('off')\n",
    "\n",
    "        axes[r, 2].imshow(pretrained_pred[0, 0].detach().cpu().numpy(), cmap='gray')\n",
    "        axes[r, 2].set_title(f'Pre MSE={mse_pre:.4f}')\n",
    "        axes[r, 2].axis('off')\n",
    "\n",
    "        axes[r, 3].imshow(finetuned_pred[0, 0].detach().cpu().numpy(), cmap='gray')\n",
    "        axes[r, 3].set_title(f'FT MSE={mse_ft:.4f}')\n",
    "        axes[r, 3].axis('off')\n",
    "\n",
    "metrics_df = pd.DataFrame(rows)\n",
    "display(metrics_df.head(20))\n",
    "\n",
    "avg_pre = metrics_df['mse_pretrained'].mean()\n",
    "avg_ft = metrics_df['mse_finetuned'].mean()\n",
    "avg_imp = metrics_df['improvement_percent'].mean()\n",
    "\n",
    "print(f'Average MSE (Pre-trained): {avg_pre:.6f}')\n",
    "print(f'Average MSE (Fine-tuned):  {avg_ft:.6f}')\n",
    "print(f'Average Improvement (%):   {avg_imp:.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / f'eval_grid_{MODE.lower()}.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "if len(ft_loss_traces) > 0:\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    for i, trace in enumerate(ft_loss_traces[:3]):\n",
    "        plt.plot(trace['losses'], label=f\"Sample {trace['sample_id']}\")\n",
    "    plt.title('Fine-tuning Loss Traces')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Physics Loss (MSE)')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / f'finetune_losses_{MODE.lower()}.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "metrics_df.to_csv(RESULTS_DIR / f'metrics_{MODE.lower()}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b4477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Export results and models as ZIP\n",
    "# =============================\n",
    "bundle_root = BASE_DIR / 'FPR_Results_Bundle'\n",
    "if bundle_root.exists():\n",
    "    shutil.rmtree(bundle_root)\n",
    "bundle_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "bundle_results = bundle_root / 'results'\n",
    "bundle_models = bundle_root / 'models'\n",
    "\n",
    "if RESULTS_DIR.exists():\n",
    "    shutil.copytree(RESULTS_DIR, bundle_results, dirs_exist_ok=True)\n",
    "if MODELS_DIR.exists():\n",
    "    shutil.copytree(MODELS_DIR, bundle_models, dirs_exist_ok=True)\n",
    "\n",
    "zip_base = BASE_DIR / 'FPR_Results'\n",
    "zip_file = shutil.make_archive(str(zip_base), 'zip', root_dir=bundle_root)\n",
    "print('Created zip:', zip_file)\n",
    "\n",
    "display(FileLink('FPR_Results.zip'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
